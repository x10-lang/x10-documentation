We now demonstrate that several common idioms of concurrency and
communication lie in Safe X10.

\subsection{Example use of \code{Acc}}
\begin{example}[Histogram]
The histogram problem is easily represented with a \code{Rail} of
accumulators: 
\begin{lstlisting}
 @safe
 def histogram(N:Int, A:Rail[Int(0..N)]):Rail[Int](N+1) {
    val result = new Rail[Acc[Int]](N+1, (Int)=>new Acc[Int](0,Int.+));
    finish for (i in A.values()) async {
       result(i) <- 1;
    }
    return new Rail[Int](N+1, (i:Int)=> result(i));
 }
\end{lstlisting}
\end{example}
Note that programs that accumulate a figure of merit can be written
with accumulators. For instance the N-Queen program needs to use a
single accumulator for collecting the result. Indeed it can be written
with the even more restrictive collecting finish style discussed
below. Similarly for Monte-Carlo simulations in which the main
activity spawns several children activities to perform a simulation,
and the simulation returns results which can be accumulated to
obtain the final answer.

Map Reduce applications such as word-count (that do not require
intermediate sorting of results) can also be expressed directly:
\begin{example}[Distributed word-count]
Here the difference from histogram is that the accumulation may be
done at a remote place. 
\begin{lstlisting}
 @safe
 def wordCount(m:DistStream[Word]):DistHashMap[Word,Int](m.dist) {
   val a = new DistHashMap[Word, Acc[Int]](m.dist,
          (w:Word)=> new Acc[Int](Int.Sum)));
   finish for (p in m.dist.places()) async at(p) {
      for (word in m(p).words())
          a(word)<- 1;
   }
   return  new DistHashMap[Word, Int](m.dist, (w:Word)=> a(w));
}
\end{lstlisting}

\end{example}


Accumulators can be used to implement collective operations such as
distributed reductions in a straightforward ``shared memory'' style.

\begin{example}[Reduction]
Here we show the single-sided, blocking version. Spawned activities
update an accumulator created by the parent activity, which reads it
after a \code{finish}. 
  \begin{lstlisting}
@safe
def reduce[T](in:DistArray[T], red:Reducible[T]):T {
    val acc = new Acc[T](red);
    finish for (dest in in.dist.places())
        async
            at (dest) 
               for (p in dist | here)
                 acc <- in(p);
    return acc();
}
\end{lstlisting}
\end{example}


%%An \code{allReduce} can be implemented by following the above
%%operation with a broadcast:
%%\begin{lstlisting}
%%  @safe
%%  def allReduce[T](in:DistArray[T]{self.dist==Dist.UNIQUE},
%%    red:Reducible[T], out:DistArray[T](in.dist)):void {
%%    val x = reduce(in, red);
%%    finish for (dest in out.dist.places()) async at (dest) {
%%      for (p in out.dist |here)
%%        out(p)=x;
%%    }
%%  }
%%\end{lstlisting}
%%One can write this code using a clock (to avoid two finish nests).
%%
%%The collective style requires extending clock so the advance method
%%takes arbitrary args and performs collective operations on them,
%%mimicking the MPI API.

\paragraph{Collecting \code{finish}}

The collecting \code{finish} construct is of the form \code{finish(r)
  S}, where \code{r} is an instance of \code{Reducible[T]} and
\code{S} is a statement. Within the dynamic execution of \code{S}, any
execution of the statement \code{offer t;} results in a value \code{t}
being accumulated in a set. (\code{t} must be of type \code{T}.) The
result of the reduction of this set of \code{T} values with \code{r}
is then returned.

\begin{lstlisting}
  {
    val x = new Acc[T](r);
    finish {
      S [ x <- t / offer t;]
    }
    x()
  }
\end{lstlisting}

An attractive aspect of collecting finish is that nesting is used
quite naturally to reflect the relationship between the parallel
computation performing the accumulation and the value returned. In
particular there is no need to explicitly introduce the notion of an
accumulator, or to register it with the current block, or to check
that other activities in the current block have quiesced.

On the other hand, this strength is also a limitation. It is not
possible to use the same idiom for clocked code, i.e.{} collect values
being offered by multiple \code{clocked} asyncs in the current phase.
Further, using this idiom safely across method calls requires the
addition of \code{offers T} clauses (similar to \code{throws T})
clauses, specifying the type of the value being offered. Otherwise at
run-time a value of an incompatible type may be offered leading to a
run-time exception. Finally, the lack of a name for the result being
collecting means that it is difficult to use the same computation to
accumulate multiple separate values without more contortions. e.g.{}
one could set the return type to be a tuple of values, but then the
\code{offer} statement would need to specify the index for which the
\code{offer} was intended. Now it is not clear that the index type
should be arithmetic -- why should not one be able to collect into the
range of a \code{HashMap} (so the index type could be an arbitrary
type \code{Key})?

We feel that these decisions are all orthogonal to the actual process
of accumulating and should be dealt with by the data-structuring
aspects of the language design.

\subsection{Clocked computations}

\begin{example}[Pipeline]
This example creates $3$ stages in parallel. Stage $1$ gets the input
(in this case generating the natural numbers), Stage $2$ increments it
by $1$, Stage $3$ doubles it and prints the value.
\begin{lstlisting}
@safe
def pipeline() {
    val a = new Clocked[Int](0,0);
    val b = new Clocked[Int](0,0);
    val c = new Clocked[Int](0,0);
    clocked finish {
        clocked async {
            for (var i:Int=0;; i++) {
                a()=i;
                advance;}}
        clocked async {
            advance;
            for (;;) {
                b()= a()+1; // b is 1,2,3,...
                advance;}}
        advance;
        advance;
        for(;;) {
            c()= 2*b();
            Console.OUT.println(c()); // Prints 2,4,6,...
            advance;
        }}}
  \end{lstlisting}
Note the serial schedule generates:
\begin{lstlisting}
  a()=0; advance;
  a()=1; b()=1; advance;
  a()=2; b()=2; c()=2; advance;
  a()=3; b()=3; c()=4; advance;
  ...
\end{lstlisting}
\noindent This shows the pipelined nature of the computation. To show
tht this program is safe, the compiler needs to establish that the
first activity ``owns'' \code{a}, the second \code{b} and the third \code{c}.
\end{example}

\begin{example}[Butterfly idiom]
This program uses a butterfly idiom to compute an all to all
reduction.
\begin{lstlisting}
def reduce[T](in:DistArray[T], red:Reducible[T]) {
    val P = Place.MAX_PLACES;
    val phases = Utils.log2(P);
    val x = new DistArray[ClockedAcc[T]](in.dist,
      (p:Point(in.rank))=>new ClockedAcc[T](in(p),in(p),red));
    clocked finish  {
        for (p in x.dist.places()) clocked async at(p) {
            var shift:Int=1;
            for (phase in 0..phases-1) {
                        x((p.id+shift)%P)() <- x(p.id)();
                shift *=2;
                advance;
            }}}
    return x(0)();
}
\end{lstlisting}
\end{example}

\input {examples/stencil.x10}

A {\em serial} schedule for a parallel program is one which always
executes the first enabled step in program order. A {\em safe}
parallel program is one that can be executed with a serial schedule
$S$ and for which for every input every schedule produces the same
result (error, correct termination, divergence) as $S$.  Such a
program is semantically a sequential program, hence it is
scheduler-determinate and deadlock-free.

A {\em safe parallel programming language} is an imperative parallel
programming language in which every legal program is a safe
program. Programmers can write code in such a language secure in the
knowledge that they will not encounter a large class of parallel
programming problems. Such a language is particularly useful for
parallelizing sequential (imperative) programs. In such cases ({\em
  contra} reactive programming) the desired application semantics are
sequential, and parallelism is needed solely for efficient
implementation.

The characteristic property of a safe programming language is that the
{\em same} program has a sequential reading and a parallel reading,
and both are compatible with each other. Hence the program can be
developed and debugged as a sequential program, using the serial
scheduler, and then run the unchanged program in parallel.  Parallel
execution is guaranteed to effect only performance, not
correctness. Safety is a very strong property.

One way to get safety is through implicitly parallel languages (e.g.{}
Jade~\cite{Rinard98thedesign},\cite{vonPraun:2007:IPO:1229428.1229443}). One
starts with a sequential programming language, and adds constructs
(e.g.{} tasks) that permit speculative execution while guaranteeing
that the only observable write to a shared variable is the write by
the last task to execute in program order.  While this work is
promising, extracting usable parallelism from a wide variety of
sequential programs remains very hard.

Explicitly parallel programming languages provide a variety of
constructs for spawning tasks in parallel and coordinating between
them. Here the programmer can typically directly control the
granularity of concurrency, and locality of access (e.g. placement of
data-structures in a multi-node computation) and use efficient
concurrent primitives (atomic reads/writes, test and sets, locks etc)
to control their execution.

For such languages proving that a program is safe -- much less that
the programming language is safe -- now becomes very hard,
particularly for modern object-oriented languages which allow the
programmer to create arbitrarily complicated data-structures in the
shared heap. It becomes very difficult to show that any possible
schedule will produce the same result as the serial schedule.

Our starting point is the language X10 \cite{x10} since it offers a
simple and elegant treatment of concurrency and distribution, with
some nice properties.  In brief, X10
introduces the constructs \code{async S} to spawn an activity to
execute \code{S}; \code{finish S} to execute \code{S} and wait until
such time as all activities spawned during its execution have
terminated; \code{at (p) S} to execute \code{S} at place
\code{p}. These constructs can be nested arbitrarily -- this is a
source of significant elegance and power. Additionally, X10 v 2.2
introduces a simplified version of X10 clocks (adequate for many
practical usages) -- \code{clocked finish S} and \code{clocked async
  S}. Briefly, a clocked finish introduces a new barrier that can be
used by this activity and its children activities for
synchronization.\footnote{X10 also has a conditional atomic construct,
  \code{when (c) S} which permits data-dependent synchronization and
  can introduce deadlocks. We do not consider this construct in this paper.}

\cite{vj-clock} establishes that a large class of programs
in X10, namely those that use \code{finish}, \code{async} and
\code{clock}s are deadlock-free. The central intuition is that a clock
can only be used by the activity that created it and by its children,
and hence the spawn tree structure can be used to avoid depends-for
cycles.

To understand the conditions under which such programs may be safe we
need to understand what it means to execute such a program with a
serial schedule. In a serial schedule, the statement \code{async
  \{S1\} S2} is executed by first executing \code{S1}, and then, on
its completion, executing \code{S2} (since the first enabled step of
\code{async \{S1\} S2} in program order will be the first enabled step
of \code{async \{S1\}}.  That is, \code{async S1} behaves like
\code{S1}. Similarly, \code{finish \{S1\} S2} is executed by first
executing \code{S1} to completion and then executing \code{S2}. Thus
under a serial schedule, \code{finish} and \code{async} behave like
``no-ops''.

The situation is slightly different for \code{clocked finish} and
\code{clocked async}. The first enabled step of \code{clocked finish
  \{S1\} S2} is going to be the first step of \code{S1}. However, the
first enabled step of \code{clocked async \{S1\} S2} may be the first
step of \code{S2} -- e.g.{} when \code{S1} is \code{advance; S3} and
the clocked async must wait until all other such asyncs in the scope
of the clocked finish are ready to advance. Thus a \code{clocked
  finish \{S\}} exhibits ``round-robin'' scheduling of \code{S}. For
instance, through this reasoning we can establish that
\begin{lstlisting}
  clocked finish {
    clocked async { S1; advance; S3}
    clocked async { S2; advance; S4}
  }
  S5
\end{lstlisting}
would give the same result as the program: \code{S1;S2;S3;S4;S5}.

When is such a program safe? The central problem
is to ensure that in the statement \code{async \{S\} S1} it is not the
case that \code{S} can write to a location that \code{S1} can read
from or read from a location that \code{S} can read from. Otherwise
the behavior is not equivalent to a serial schedule. One line of attack has been the pursuit
of {\em effect systems} \cite{Lucassen:1988:PES:73560.73564},
\cite{Leino:2002:UDG:543552.512559},
\cite{boyland:01interdependence},
\cite{DPJ}. Broadly, static effects systems call
for a user-specified partitioning of heap into {\em regions} in a
fine-grained enough way to show that operations that may occur
simultaneously work on different regions. For instance \cite{DPJ}
introduces separate syntax for regions, introduces the ability to
specify an arbitrary tree of regions, and new syntax for specifying
which region mutable locations belong to. Methods must be associated
with their read and write effects which capture the set of regions
that can be operated upon during the execution of this
method. Now determinacy can be established if it can be determined
that that in \code{async \{S\} S1} it is the case that \code{S} does
not write any region that \code{S1} reads or writes from, and vice
versa ({\em disjoint parallelism}).

\cite{DPJ} develops these ideas in the context of a language with
\code{cobegin/coend} and \code{forall} parallelism, and does not
address arbitrary nested or clocked parallelism.  In particular it is
not clear how to adapt these ideas to support some form of pipelined
communication between multiple parallel activities. Once can think of
these computations as requiring ``disjointness across time'' rather
than disjointness across space. A producer is going to write into
locations \code{w(t),w(t+1),w(t+2), \ldots}, but this does not
conflict with a consumer reading from the same locations as long as
the consumer can arrange to read the values in time-staggered order,
i.e. read \code{w(t)} once the produce is writing \code{w(t+1)}.

We believe it is possible to significantly simplify this approach
(e.g.{} using the dependent-type system of X10) and extend it to cover
all of X10's concurrency constructs (see
\Ref{Section}{conclusion}). Nevertheless the cost of developing these
region assertions is not trivial. Their viability for developing large
commercial-strength software systems is yet to be establishd.

In this paper we chose a different approach, a {\em lightweight}
approach to safety. We introduce two simple ideas -- {\em
  accumulators} and {\em clocked types} -- which require very modest
compiler support and can be implemented efficiently at run-time. We
show that programs written using accumulators and clocked accumulators
as the only shared variables are safe.

Accumulators arise very naturally in concurrent programming: an
accumulator is a mutable location associated with a commutative and
associative {\em reduction} operator that can be operated on
simultaneously by multiple activities. Multiple values offered by
multiple activities are combined by the reduction operator. We show
that the concrete rules for accumulators can be defined in such a way
that they do not compromise safety: a serial execution precisely
captures results generable from any execution.

Similarly clocked computations (barrier-based synchronization) is
quite common in parallel programming e.g.{} in the SPMD model, BSP model
etc. We observe that many clocked programs can be written in such a
way that shared variables take on a single value in one phase of the
clock. Further, clocked computations are often iterative and operate
on large aggregate data-structures (e.g.{} arrays, hash-maps) in a
data-parallel fashion, reading one version of the data-structure (the
``red'' version) while simultaneously writing another version (the
 ``black'' version). To support this widely used idiom, we introduce
the notion of {\em clocked types}. An instance \code{a} of a  clocked type
\code{Clocked[T]} keeps two instances of type \code{T}, the \code{now} and
the \code{next} instance. \code{a} can be operated upon only by
activities registered on the current clock.  All read operations
during the current clock phase are directed to the \code{now} version,
and write operation to the \code{next} version. This ensures that
there are no read-write conflicts. There may be write-write conflicts
-- these must be managed by either using accumulators or an effects
system. Once computation in the current phase has quiesced -- and
before activities start in the next phase -- the \code{now} and
\code{next} versions are switched; \code{now} becomes \code{next} and
\code{next} becomes \code{now}.\footnote{Clearly this idea can be
  extended to $k$-buffered clocked types where each clock tick rotates
  the buffer. This idea is related to the K-bounded Kahn networks of
\cite{Cohen:2006:NSK:1111320.1111054}.}

We show that clocked types can be defined in a safe way, provided that
accumulators are used to resolve write-write conflicts, or a light
weight ``owner computes'' rule is used to check that only the $i$th async
in a parallel loop writes into the $i$th location of an array. 
The only dynamic check needed is that a value of this type is being operated
upon only by the activity that created the value or its
descendants.

In the following by Safe X10 we shall mean the language X10 restricted
to use (\code{clocked}) \code{finish}/\code{async}, \code{at} with
(clocked) accumulators, or with clocked types where an ``owner
computes'' rule is used to establish disjointness of array writes. 
 All programs in Safe X10 are safe -- they can
be run with a serial schedule and their I/O behavior is identical
under any schedule.  We show that Safe X10 is surprisingly
powerful. Many concurrent idioms can be expressed in this language --
histograms, all-reduce, and master/slave communication.
Indeed, even some
form of pipelined/systolic communication is expressible.

Since the dynamic conditions introduced on accumulators and clocked
types are not straightforward, we formalize the concurrent and serial
semantics of an abstraction of Safe X10 using Plotkin's structural
operational style. We are able to do this in such a way that the two
proof systems share most of the proof rules, simplifying the proof. We
establish that the language is safe -- for any program and any input,
any execution sequence for the concurrent proof rules can be
transformed into an execution sequence for the sequential proof rules
with the same result.

In summary the contributions of this paper are as follows.

\begin{itemize}
\item We identify the notion of a {\em safe} program -- one which can
  be executed with a serial schedule and for which
  every schedule produces the same result. Such a program is
  simultaneously a sequential program and a parallel program with
  identical I/O behavior.
\item We introduce accumulators and clocked types in the X10
  programming model. These are introduced in such a way that arbitrary
  programs using (\code{clocked}) \code{finish}, \code{async} and
  \code{at} and in which the only variables shared between
  concurrently executing activities are accumulators or clocked
  accumulators (or clocked types with an ``owner computes'' rule to
  establish disjointness of updates) are guaranteed to be safe.
\item We show that many common programming idioms can be expressed in
  this language.
\item We formalize a fairly rich subset of X10 -- including (clocked)
  finish, async, accumulators and clocked accumulators.  This is the
  first formalization of the nested clock design of X10 2.2, and is
  substantially simpler than \cite{vj-clock}. We establish that this
  language is safe.
\end{itemize}
In companion work we show how these ideas can be extended to support
modularly defined effects analyses, using X10's dependent type system.

The rest of this paper is as follows. In \Ref{Section}{related} we discuss
related work. In \Ref{Section}{constructs} we present the constructs in detail,
followed by examples of their use. \Ref{Section}{semantics} presents the
semantics of these constructs. We discuss implementation in
\Ref{Section}{conclusion} and finally conclude with future work.

%%\cite{Gifford:1986:IFI:319838.319848}
%%
%%
%%Figure~\ref{fig:1} shows  the famous ``parallel Or'' program of Plotkin
%% (in X10 syntax, \cite{x10}). This program can be executed with a
%%depth-first schedule, is partially determinate and deadlock-free, but {\em not}
%%safe. The result of running the sequential schedule is not the same as
%%the result that can be obtained with other schedules. Specifically
%%\code{parallelOr(()=> CONT, ()=>TRUE)} will diverge (exhibit an
%%infinite exection sequence) under the depth-first schedule, but will
%%return \code{true} under any fair schedule that permits the second
%%async to progress.
%%
%%\begin{figure}
%%  \begin{lstlisting}
%%static val CONT=1, TRUE=2, FALSE=3;
%%def run(done:Cell[Boolean], a:()=>Int) {
%% var aa:Int=a();
%% var cont:Boolean=true;
%% for (; aa==CONT && cont;aa=a()) {
%%  atomic cont = !done();
%% }
%% if (aa==TRUE)
%%  atomic done()=true;
%%}
%%def parallelOr(a:()=>Int, b:()=>Int):Boolean {
%% val done=new Cell[Boolean](false);
%% finish {
%%  async run(done, a);
%%  async run(done, b);
%% }
%% return done();
%%}
%%  \end{lstlisting}
%%  \caption{A program that is not sequential}\label{fig:1}
%%\end{figure}



%%{\em
%%\begin{enumerate}
%%\item Use activity registration as a mechanism to tame object graphs.
%%\item Focus on structured concurrency. Using scoping and block-structure
%%    to delimit regions of code that may execute in parallel and affect
%%    the data structure.
%%
%%\item Accumulation can be defined safely by delaying. However, the delay
%%    operation is guaranteed to be deadlock-free.
%%
%%\item Clocked types support phased computation, another common idiom
%%    particularly for stencil computations.
%%\end{enumerate}
%%}
%%
%%Key contributions:
%%{\em
%%\begin{enumerate}
%%\item Identification of determinate, deadlock-free data-structures.
%%\item Discussion of design alternatives which points out the
%%  difficulty of integrating these ideas in a modern OO language.
%%\item Discussion of various idioms expressible using these data-structures.
%%\item Proof of determinacy and deadlock-freedom in an abstract version
%%  of the language.
%%\end{enumerate}
%%These constructs are implemented in \Xten, available as open source from
%%SVN head and will be in the next release of \Xten.
%%}


%Semantics and theorems for an abstract version of the language.
